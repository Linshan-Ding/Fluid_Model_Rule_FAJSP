# åŒæ­¥å•çº¿ç¨‹A2Cç®—æ³•ï¼ˆä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ï¼‰å®ç°ï¼ŒåŒ…å«GAEä¼˜åŠ¿ä¼°è®¡ã€n_stepæ­¥æ›´æ–°ï¼Œå¹¶æ”¯æŒè½®è®­è®­ç»ƒ
import os
import sys
import signal
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from CNN import CNNFeatureExtractor
from ATN import SelfAttention
from MLP import MLP
from env.env import FAJSP_Environment
import numpy as np
import random
import time
import copy
import visdom

# è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆå§‹åŒ–Visdom
viz = visdom.Visdom(env='FAJSP_A2C_Training')

# å…¨å±€ä¸­æ–­æ ‡å¿—
#æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·åéœ€ç­‰å¾…å…¶å®Œæˆä¸€ä¸ªè½®æ¬¡
interrupt_flag = False
def signal_handler(sig, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·ï¼ˆCtrl+F2ï¼‰"""
    global interrupt_flag
    interrupt_flag = True
    print("\næ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· (Ctrl+F2)ï¼Œæ­£åœ¨ä¿å­˜å½“å‰çŠ¶æ€...")

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)

# ç­–ç•¥ç½‘ç»œ
class PolicyNetwork(nn.Module):
    def __init__(self, action_feature_dim, combined_dim):
        super(PolicyNetwork, self).__init__()
        self.device = device

        self.cnn = CNNFeatureExtractor(input_channels=1).to(self.device)
        self.atn = SelfAttention(feature_dim=action_feature_dim, num_heads=1).to(self.device)
        self.mlp = MLP(input_dim=combined_dim).to(self.device)

    def forward(self, matrix_states, candidate_actions, candidate_actions_features):
        # CNN state features
        state_features = []
        for matrix in matrix_states:
            m = torch.tensor(matrix).unsqueeze(0).unsqueeze(0).float().to(self.device)
            feat = self.cnn(m)
            state_features.append(feat)
        state_feature = torch.cat(state_features, dim=1)

        # ATN action features
        action_feature_list = []
        for a in candidate_actions:
            action_feature_list.append(candidate_actions_features[a])
        action_tensor = torch.tensor(action_feature_list).unsqueeze(0).float().to(self.device)
        action_trans = self.atn(action_tensor)

        # Score each action
        scores = []
        for i in range(len(candidate_actions)):
            af = action_trans[:, i, :]
            combined = torch.cat([state_feature, af], dim=1)
            score = self.mlp(combined)
            scores.append(score)

        scores_tensor = torch.cat(scores, dim=1).squeeze(0)
        action_probs = F.softmax(scores_tensor, dim=0)
        return action_probs

# ä»·å€¼ç½‘ç»œ
class ValueNetwork(nn.Module):
    def __init__(self, mlp_value_dim):
        super(ValueNetwork, self).__init__()
        self.device = device

        self.cnn = CNNFeatureExtractor(input_channels=1).to(self.device)
        self.mlp = MLP(input_dim=mlp_value_dim).to(self.device)

    def forward(self, matrix_states):
        feats = []
        for matrix in matrix_states:
            m = torch.tensor(matrix).unsqueeze(0).unsqueeze(0).float().to(self.device)
            feat = self.cnn(m)
            feats.append(feat)
        state_feature = torch.cat(feats, dim=1)
        return self.mlp(state_feature)

# ACAgent with GAE
class A2CAgent:
    def __init__(self, action_feature_dim, combined_dim, mlp_value_dim,
                 policy_lr=1e-3, value_lr=1e-3, gamma=0.99, gae_lambda=0.95, n_steps=5):
        self.device = device
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.n_steps = n_steps # næ­¥æ›´æ–°

        self.policy_net = PolicyNetwork(action_feature_dim, combined_dim).to(self.device)
        self.value_net = ValueNetwork(mlp_value_dim).to(self.device)

        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)
        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=value_lr)

        self.transitions = []

        # æœ€ä¼˜å‚æ•°è®°å½•
        self.best_policy_state = None
        self.best_value_state = None
        self.best_completion_time = float('inf')

        # å…¨å±€æœ€ä¼˜å‚æ•°è®°å½•ï¼ˆç”¨äºè½®è®­è®­ç»ƒï¼‰
        self.global_best_policy_state = None
        self.global_best_value_state = None
        self.global_best_avg_ct = float('inf')
        self.global_best_round = -1

    # æ›´æ–°æœ€ä¼˜å‚æ•°ï¼ˆåŸºäºå•æ¬¡å®Œå·¥æ—¶é—´ï¼‰ç”¨äºä¿å­˜pthæ–‡ä»¶
    def update_best_params(self, completion_time):
        if completion_time < self.best_completion_time:
            self.best_completion_time = completion_time
            self.best_policy_state = copy.deepcopy(self.policy_net.state_dict())
            self.best_value_state = copy.deepcopy(self.value_net.state_dict())
            return True
        return False

    # æ›´æ–°å…¨å±€æœ€ä¼˜å‚æ•°ï¼ˆåŸºäºè½®æ¬¡å¹³å‡å®Œå·¥æ—¶é—´ï¼‰ç”¨äºä¿å­˜å…¨å±€æœ€ä¼˜pthæ–‡ä»¶
    def update_global_best_params(self, avg_completion_time, round_num):
        """æ›´æ–°å…¨å±€æœ€ä¼˜å‚æ•°ï¼ˆåŸºäºè½®æ¬¡å¹³å‡å®Œå·¥æ—¶é—´ï¼‰"""
        if avg_completion_time < self.global_best_avg_ct:
            self.global_best_avg_ct = avg_completion_time
            self.global_best_policy_state = copy.deepcopy(self.policy_net.state_dict())
            self.global_best_value_state = copy.deepcopy(self.value_net.state_dict())
            self.global_best_round = round_num
            return True
        return False

    def take_action(self, matrix_states, candidate_actions, candidate_actions_features):
        """é€‰æ‹©åŠ¨ä½œ - è¿”å›åŠ¨ä½œç´¢å¼•å’ŒçŠ¶æ€ä»·å€¼"""
        with torch.no_grad():
            # è·å–åŠ¨ä½œæ¦‚ç‡
            action_probs = self.policy_net(matrix_states, candidate_actions, candidate_actions_features)
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample()
            action_idx = action.item()

            # è·å–çŠ¶æ€ä»·å€¼
            state_value = self.value_net(matrix_states)
            if state_value.dim() > 0:
                state_value = state_value.squeeze()

        return action_idx, state_value.item()

    def store_transition(self, state, action_idx, state_value, reward, next_state, done,
                         candidate_actions, candidate_actions_features):
        """å­˜å‚¨ç»éªŒ"""
        self.transitions.append({
            'states': state,
            'action_idx': action_idx,
            'state_value': state_value,
            'rewards': reward,
            'next_states': next_state,
            'dones': done,
            'candidate_actions': candidate_actions,
            'candidate_actions_features': candidate_actions_features
        })
    #ä¸åŸºç¡€ACAgentä¸åŒï¼ŒA2Cä½¿ç”¨GAEä¼˜åŠ¿ä¼°è®¡
    def compute_gae_advantages(self, rewards, values, next_values, dones):
        """è®¡ç®—GAEä¼˜åŠ¿"""
        advantages = []
        gae = 0

        for t in reversed(range(len(rewards))):
            if dones[t]:
                delta = rewards[t] - values[t]
            else:
                delta = rewards[t] + self.gamma * next_values[t] - values[t]

            gae = delta + self.gamma * self.gae_lambda * gae
            advantages.insert(0, gae)

        return torch.stack(advantages)

    def update(self):
        """æ›´æ–°ç½‘ç»œ"""
        if len(self.transitions) == 0:
            return 0.0, 0.0

        # 1. å‡†å¤‡æ•°æ®
        states = [t['states'] for t in self.transitions]
        action_idxs = [t['action_idx'] for t in self.transitions]
        rewards = torch.tensor([t['rewards'] for t in self.transitions],
                               dtype=torch.float32).to(self.device)
        next_states = [t['next_states'] for t in self.transitions]
        dones = torch.tensor([t['dones'] for t in self.transitions],
                             dtype=torch.float32).to(self.device)

        candidate_actions_list = [t['candidate_actions'] for t in self.transitions]
        candidate_actions_features_list = [t['candidate_actions_features'] for t in self.transitions]

        # 2. è®¡ç®—å½“å‰çŠ¶æ€ä»·å€¼
        state_values = []
        for s in states:
            value = self.value_net(s)
            if value.dim() > 0:
                value = value.squeeze()
            state_values.append(value)
        state_values = torch.stack(state_values)

        # 3. è®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€ä»·å€¼
        next_state_values = []
        for ns in next_states:
            with torch.no_grad():
                next_value = self.value_net(ns)
                if next_value.dim() > 0:
                    next_value = next_value.squeeze()
                next_state_values.append(next_value)
        next_state_values = torch.stack(next_state_values)

        # 4. è®¡ç®—GAEä¼˜åŠ¿
        advantages = self.compute_gae_advantages(
            rewards, state_values.detach(), next_state_values, dones
        )

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # 5. è®¡ç®—å›æŠ¥ï¼ˆç”¨äºCriticè®­ç»ƒï¼‰
        returns = advantages + state_values.detach()

        # 6. é‡æ–°è®¡ç®—log_probsä»¥ä¿æŒè®¡ç®—å›¾
        log_probs = []
        for i in range(len(states)):
            action_probs = self.policy_net(
                states[i],
                candidate_actions_list[i],
                candidate_actions_features_list[i]
            )
            log_prob = torch.log(action_probs[action_idxs[i]] + 1e-10)
            log_probs.append(log_prob)
        log_probs = torch.stack(log_probs)

        # 7. è®¡ç®—ç­–ç•¥æŸå¤±
        actor_loss = -torch.mean(log_probs * advantages)

        # 8. è®¡ç®—ä»·å€¼æŸå¤±
        critic_loss = F.mse_loss(state_values, returns.detach())

        # 9. æ›´æ–°ç­–ç•¥ç½‘ç»œ
        self.policy_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=0.5)
        self.policy_optimizer.step()

        # 10. æ›´æ–°ä»·å€¼ç½‘ç»œ
        self.value_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), max_norm=0.5)
        self.value_optimizer.step()

        # æ¸…ç©ºç»éªŒ
        self.transitions.clear()

        return actor_loss.item(), critic_loss.item()

# å•ä¸ªç®—ä¾‹çš„è®­ç»ƒå™¨,ç”¨äºè½®è®­è®­ç»ƒ
class InstanceTrainer:
    def __init__(self, data_config, viz_windows, n_steps=5):
        self.data_config = data_config
        self.config_str = f"{data_config[0]}_{data_config[1]}_{data_config[2]}_{data_config[3]}_{data_config[4]}_{data_config[5]}"
        self.viz_windows = viz_windows # Visdomçª—å£å­—å…¸
        self.n_steps = n_steps

        # åˆ›å»ºç¯å¢ƒå’ŒA2C Agent
        M_p, M_a, product_count, kind_count, J_r, N_p = data_config

        self.env = FAJSP_Environment(
            M_p=M_p, M_a=M_a,
            kind_count=kind_count,
            product_count=product_count,
            J_r=J_r, N_p=N_p
        )
        self.env.reset()

        # è·å–åŠ¨ä½œç‰¹å¾ç»´åº¦
        ca = self.env.get_candidate_actions()
        if len(ca) == 0:
            raise ValueError("é”™è¯¯ï¼šæ²¡æœ‰å€™é€‰åŠ¨ä½œ")

        caf = self.env.action_features(ca[0])
        feature_dim = len(caf[ca[0]])

        # è·å– CNN çŠ¶æ€ç»´åº¦
        cnn = CNNFeatureExtractor(input_channels=1).to(device)
        s = self.env.current_state
        feats = []
        for m in s:
            t = torch.tensor(m).unsqueeze(0).unsqueeze(0).float().to(device)
            feats.append(cnn(t))
        sf = torch.cat(feats, dim=1)
        state_dim = sf.shape[1]

        # åˆ›å»ºA2C Agent
        self.agent = A2CAgent(
            action_feature_dim=feature_dim,
            combined_dim=state_dim + feature_dim,
            mlp_value_dim=state_dim,
            gamma=0.99,
            gae_lambda=0.95,
            n_steps=n_steps
        )

        # è®°å½•æ•°æ®
        self.episode_rewards = []
        self.completion_times = []
        self.policy_losses = []
        self.value_losses = []

        self.episode_count = 0
        self.best_completion_time = float('inf')
        self.best_schedule_log = None

    def train_one_episode(self):
        self.env.reset()
        state = self.env.current_state
        total_reward = 0

        while True:
            candidate_actions = self.env.get_candidate_actions()
            if not candidate_actions:
                break

            candidate_actions_features = {
                a: self.env.action_features(a)[a]
                for a in candidate_actions
            }

            # æ”¶é›†næ­¥ç»éªŒ,ä¸åŸºç¡€ACAgentä¸åŒï¼ŒACä¸€ä¸ªå‘¨æœŸè¿›è¡Œä¸€æ¬¡è¿›è¡Œæ›´æ–°ï¼ŒA2Cåˆ™æ˜¯æ¯næ­¥æ›´æ–°ä¸€æ¬¡
            for step in range(self.n_steps):
                # é€‰æ‹©åŠ¨ä½œ
                action_idx, state_value = self.agent.take_action(
                    state, candidate_actions, candidate_actions_features
                )
                action = candidate_actions[action_idx]

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done = self.env.step(action)
                total_reward += reward

                # å­˜å‚¨ç»éªŒ
                self.agent.store_transition(
                    state=state,
                    action_idx=action_idx,
                    state_value=state_value,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    candidate_actions=candidate_actions,
                    candidate_actions_features=candidate_actions_features
                )

                state = next_state

                if done:
                    break

                # æ›´æ–°å€™é€‰åŠ¨ä½œ
                candidate_actions = self.env.get_candidate_actions()
                if not candidate_actions:
                    break

                candidate_actions_features = {
                    a: self.env.action_features(a)[a]
                    for a in candidate_actions
                }

            # æ›´æ–°ç½‘ç»œ
            actor_loss, critic_loss = self.agent.update()

            if done:
                break

        # è®°å½•æ•°æ®
        self.episode_rewards.append(total_reward)
        self.completion_times.append(self.env.completion_time)
        #å¯ç”¨äºç»˜å›¾
        self.policy_losses.append(actor_loss)
        self.value_losses.append(critic_loss)

        current_ct = float(self.env.completion_time)

        # æ›´æ–°æœ€ä¼˜è®°å½•
        if current_ct < self.best_completion_time:
            self.best_completion_time = current_ct
            self.best_schedule_log = copy.deepcopy(getattr(self.env, "schedule_log", []))
            self.agent.update_best_params(current_ct)

        # æ›´æ–°Visdomï¼Œä¸ä»¥0ä¸ºåŸºç‚¹ç»˜å›¾
        if self.episode_count == 0:
            viz.line(
                X=np.array([self.episode_count]),
                Y=np.array([current_ct]),
                win=self.viz_windows['ct_win'],
                update='replace',
                opts=dict(
                    title=f'Completion Time - {self.config_str}',
                    xlabel='Episode',
                    ylabel='Completion Time'
                )
            )
        else:
            viz.line(
                X=np.array([self.episode_count]),
                Y=np.array([current_ct]),
                win=self.viz_windows['ct_win'],
                update='append',
                opts=dict(title=f'Completion Time - {self.config_str}', xlabel='Episode', ylabel='Completion Time')
            )

        self.episode_count += 1

        return total_reward, current_ct, actor_loss, critic_loss

    # ä¿å­˜ä¸­æ–­ç»“æœ
    def save_interrupt_results(self, round_num=None):
        config_str = self.config_str
        interrupt_path = f"a2c_fajsp_model_{config_str}.pth"

        kind_task_tuple = getattr(self.env, 'kind_task_tuple', [])
        kind_task_tuple_r = getattr(self.env, 'kind_task_tuple_r', [])
        kind_task_tuple_a = getattr(self.env, 'kind_task_tuple_a', [])
        machine_tuple = getattr(self.env, 'machine_tuple', [])

        save_data = {
            'config': self.data_config,
            'episode_count': self.episode_count,
            'current_policy': self.agent.policy_net.state_dict(),
            'current_value': self.agent.value_net.state_dict(),
            'policy_optimizer': self.agent.policy_optimizer.state_dict(),
            'value_optimizer': self.agent.value_optimizer.state_dict(),
            'best_policy': self.agent.best_policy_state,
            'best_value': self.agent.best_value_state,
            'reward': self.episode_rewards,
            'completion_times': self.completion_times,
            'best_completion_time': self.agent.best_completion_time,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'best_schedule_log': self.best_schedule_log,
            'gantt_info': {
                'kind_task_tuple': kind_task_tuple,
                'kind_task_tuple_r': kind_task_tuple_r,
                'kind_task_tuple_a': kind_task_tuple_a,
                'machine_tuple': machine_tuple
            },
            'interrupt_round': round_num,
            'interrupt_time': time.strftime("%Y-%m-%d %H:%M:%S")
        }

        torch.save(save_data, interrupt_path)
        return interrupt_path

    # æ­£å¸¸ç»“æŸä¿å­˜æœ€ç»ˆç»“æœ
    def save_results(self):
        config_str = self.config_str
        save_path = f"a2c_fajsp_model_{config_str}.pth"

        kind_task_tuple = getattr(self.env, 'kind_task_tuple', [])
        kind_task_tuple_r = getattr(self.env, 'kind_task_tuple_r', [])
        kind_task_tuple_a = getattr(self.env, 'kind_task_tuple_a', [])
        machine_tuple = getattr(self.env, 'machine_tuple', [])

        torch.save({
            'config': self.data_config,
            'policy': self.agent.best_policy_state,
            'value': self.agent.best_value_state,
            'reward': self.episode_rewards,
            'completion_times': self.completion_times,
            'best_completion_time': self.agent.best_completion_time,
            'policy_losses': self.policy_losses,
            'value_losses': self.value_losses,
            'best_schedule_log': self.best_schedule_log,
            'gantt_info': {
                'kind_task_tuple': kind_task_tuple,
                'kind_task_tuple_r': kind_task_tuple_r,
                'kind_task_tuple_a': kind_task_tuple_a,
                'machine_tuple': machine_tuple
            }
        }, save_path)

        return save_path

# è½®è®­è®­ç»ƒä¸»ç¨‹åº
def train_round_robin(data_configs, total_episodes_per_instance=10000, n_steps=5):
    global interrupt_flag

    # è®¡ç®—æ€»è®­ç»ƒå‘¨æœŸæ•°
    total_episodes = len(data_configs) * total_episodes_per_instance

    # ä¸ºæ¯ä¸ªç®—ä¾‹åˆ›å»ºvisdomçª—å£
    trainers = []
    for data_config in data_configs:
        config_str = f"{data_config[0]}_{data_config[1]}_{data_config[2]}_{data_config[3]}_{data_config[4]}_{data_config[5]}"

        viz_windows = {
            'ct_win': viz.line(
                X=np.array([0]),
                Y=np.array([0]),
                opts=dict(
                    title=f'Completion Time - {config_str}',
                    xlabel='Episode',
                    ylabel='Completion Time',
                    env='FAJSP_A2C_Training',
                    showlegend=False
                )
            )
        }

        trainer = InstanceTrainer(data_config, viz_windows, n_steps=n_steps)
        trainers.append(trainer)

    # åˆ›å»ºå¹³å‡å®Œå·¥æ—¶é—´çš„Visdomçª—å£ï¼Œå¯æœ‰å¯ä¸æœ‰
    avg_ct_win = viz.line(
        X=np.array([0]),
        Y=np.array([0]),
        opts=dict(
            title='Round Average Completion Time (A2C)',
            xlabel='Round',
            ylabel='Average CT',
            env='FAJSP_A2C_Training',
            showlegend=False
        )
    )

    # è½®è®­è®­ç»ƒ
    start_time = time.time()
    global_episode = 0
    bar_length = 40

    # æ·»åŠ è½®æ¬¡è®°å½•
    round_num = 0
    round_avg_ct_list = []

    # æ˜¾ç¤ºåˆå§‹è¿›åº¦æ¡ï¼Œå¤šè½®æ¬¡1æ›´æ–°
    bar = 'â–‘' * bar_length
    print(f"å…¨å±€è¿›åº¦: {bar} 0.0% (0/{total_episodes})", end='', flush=True)

    # è·å–ç¬¬ä¸€ä¸ªtrainerçš„agentå¼•ç”¨ï¼ˆç”¨äºå…¨å±€æœ€ä¼˜æ¨¡å‹ï¼‰
    global_agent = trainers[0].agent if trainers else None

    try:
        while True:
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupt_flag:
                print("\nä¿å­˜ä¸­æ–­çŠ¶æ€...")
                for trainer in trainers:
                    trainer.save_interrupt_results(round_num)

                # ä¿å­˜å…¨å±€ä¸­æ–­çŠ¶æ€
                if global_agent:
                    interrupt_global_path = "a2c_fajsp_model_global.pth"
                    torch.save({
                        'global_best_policy_state': global_agent.global_best_policy_state,
                        'global_best_value_state': global_agent.global_best_value_state,
                        'global_best_avg_ct': global_agent.global_best_avg_ct,
                        'global_best_round': global_agent.global_best_round,
                        'round_avg_ct_history': round_avg_ct_list,
                        'interrupt_round': round_num,
                        'total_global_episodes': global_episode,
                        'interrupt_time': time.strftime("%Y-%m-%d %H:%M:%S")
                    }, interrupt_global_path)

                print("ä¸­æ–­çŠ¶æ€å·²ä¿å­˜ï¼Œç¨‹åºé€€å‡ºã€‚")
                break

            # æ£€æŸ¥æ–‡ä»¶ä¸­æ–­
            if os.path.exists("STOP_a2c.txt"):
                print("\næ£€æµ‹åˆ°STOP_a2c.txtæ–‡ä»¶ï¼Œä¿å­˜çŠ¶æ€...")
                for trainer in trainers:
                    trainer.save_interrupt_results(round_num)

                # ä¿å­˜å…¨å±€ä¸­æ–­çŠ¶æ€
                if global_agent:
                    interrupt_global_path = "a2c_fajsp_model_global.pth"
                    torch.save({
                        'global_best_policy_state': global_agent.global_best_policy_state,
                        'global_best_value_state': global_agent.global_best_value_state,
                        'global_best_avg_ct': global_agent.global_best_avg_ct,
                        'global_best_round': global_agent.global_best_round,
                        'round_avg_ct_history': round_avg_ct_list,
                        'interrupt_round': round_num,
                        'total_global_episodes': global_episode,
                        'interrupt_time': time.strftime("%Y-%m-%d %H:%M:%S")
                    }, interrupt_global_path)

                os.remove("STOP_a2c.txt")
                break

            all_done = all(t.episode_count >= total_episodes_per_instance for t in trainers)
            if all_done:
                break

            # æ¯ä¸ªè½®æ¬¡å¼€å§‹å‰é‡ç½®è®¡æ•°å™¨
            round_total_ct = 0
            round_instance_count = 0

            # è½®æµè®­ç»ƒï¼šæ¯ä¸ªç®—ä¾‹è®­ç»ƒ1ä¸ªå‘¨æœŸ
            for trainer in trainers:
                if trainer.episode_count >= total_episodes_per_instance:
                    continue

                total_reward, current_ct, pol_loss, val_loss = trainer.train_one_episode()

                round_total_ct += current_ct
                round_instance_count += 1
                global_episode += 1

            # è®¡ç®—æœ¬è½®æ¬¡çš„å¹³å‡å®Œå·¥æ—¶é—´
            if round_instance_count > 0:
                round_avg_ct = round_total_ct / round_instance_count
                round_avg_ct_list.append(round_avg_ct)

                # æ›´æ–°å¹³å‡CT Visdomæ›²çº¿
                viz.line(
                    X=np.array([round_num]),
                    Y=np.array([round_avg_ct]),
                    win=avg_ct_win,
                    update='append',
                    opts=dict(title='Round Average Completion Time (A2C)', xlabel='Round', ylabel='Average CT')
                )

                # æ›´æ–°è¿›åº¦æ¡
                progress = min(100, global_episode / total_episodes * 100)
                filled_length = int(bar_length * progress / 100)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                print(f"\rå…¨å±€è¿›åº¦: {bar} {progress:.1f}% ({global_episode}/{total_episodes})", end='', flush=True)

                round_num += 1

    except Exception as e:
        print(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    if not interrupt_flag:
        for trainer in trainers:
            save_path = trainer.save_results()
            print(f"âœ… {trainer.config_str}: æœ€ä¼˜å®Œå·¥æ—¶é—´: {trainer.agent.best_completion_time:.2f}")

        # ä¿å­˜å…¨å±€æœ€ä¼˜æ¨¡å‹
        if global_agent and global_agent.global_best_policy_state is not None:
            global_best_path = "a2c_fajsp_global_best.pth"
            torch.save({
                'global_best_policy_state': global_agent.global_best_policy_state,
                'global_best_value_state': global_agent.global_best_value_state,
                'global_best_avg_ct': global_agent.global_best_avg_ct,
                'global_best_round': global_agent.global_best_round,
                'round_avg_ct_history': round_avg_ct_list
            }, global_best_path)
            print(f"\nğŸ¯ å…¨å±€æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜: {global_best_path}")
            print(
                f"   æœ€ä¼˜å¹³å‡å®Œå·¥æ—¶é—´: {global_agent.global_best_avg_ct:.2f} (ç¬¬ {global_agent.global_best_round} è½®)")

    total_time = time.time() - start_time

    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}ç§’")
    print("å„é…ç½®æœ€ä¼˜å®Œå·¥æ—¶é—´:")
    for trainer in trainers:
        print(f"  {trainer.config_str}: {trainer.agent.best_completion_time:.2f}")
    print("=" * 60)

    return trainers

if __name__ == "__main__":
    # æ‰€æœ‰ç®—ä¾‹é…ç½®
    data_list = [
        [4, 2, 2, 4, 2, 2], [4, 2, 2, 4, 2, 4], [4, 2, 2, 4, 2, 6],
        [4, 2, 4, 8, 4, 2], [4, 2, 4, 8, 4, 4], [4, 2, 4, 8, 4, 6],
        [4, 2, 6, 12, 6, 2], [4, 2, 6, 12, 6, 4], [4, 2, 6, 12, 6, 6],
        [8, 4, 2, 4, 2, 2], [8, 4, 2, 4, 2, 4], [8, 4, 2, 4, 2, 6],
        [8, 4, 4, 8, 4, 2], [8, 4, 4, 8, 4, 4], [8, 4, 4, 8, 4, 6],
        [8, 4, 6, 12, 6, 2], [8, 4, 6, 12, 6, 4], [8, 4, 6, 12, 6, 6],
        [12, 6, 2, 4, 2, 2], [12, 6, 2, 4, 2, 4], [12, 6, 2, 4, 2, 6],
        [12, 6, 4, 8, 4, 2], [12, 6, 4, 8, 4, 4], [12, 6, 4, 8, 4, 6],
        [12, 6, 6, 12, 6, 2], [12, 6, 6, 12, 6, 4], [12, 6, 6, 12, 6, 6]
    ]

    # æµ‹è¯•ç”¨çš„å°åˆ—è¡¨
    test_list = [[4, 2, 2, 4, 2, 2], [4, 2, 2, 4, 2, 4], [4, 2, 2, 4, 2, 6]]

    print("A2Cç®—æ³•è½®è®­è®­ç»ƒå¼€å§‹")
    print("æŒ‰ Ctrl+F2 ä¸­æ–­è®­ç»ƒ")

    # å¼€å§‹è½®è®­è®­ç»ƒ
    trainers = train_round_robin(data_list, total_episodes_per_instance=10000, n_steps=10)